{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d2e7310-99c8-4d20-9e76-7017eb985182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Go.\tGeh.\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)\n",
      "\n",
      "('Go.', 'Geh.')\n",
      "324282\n",
      "\n",
      "('Tom is fishing today.', 'Tom angelt heute.')\n",
      "('Adjust your tie.', 'Richte deine Krawatte!')\n",
      "('Who did you give my book to?', 'Wem hast du mein Buch gegeben?')\n",
      "('This book is worth reading.', 'Es lohnt sich, dieses Buch zu lesen.')\n",
      "('Tom is out shoveling snow.', 'Tom schippt draußen Schnee.')\n",
      "ENG vocab size: 8000\n",
      "GER vocab size: 8000\n",
      "Train: 308067\n",
      "Val: 16215\n",
      "Total: 324282\n",
      "src: torch.Size([64, 20])\n",
      "tgt_in: torch.Size([64, 20])\n",
      "tgt_out: torch.Size([64, 20])\n",
      "Trainable parameters: 1881408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Магістрат\\ІAД\\Lab41\\venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train loss/token: 3.4507  val loss/token: 2.3313\n",
      "Epoch 2/10 - train loss/token: 2.2529  val loss/token: 1.8451\n",
      "Epoch 3/10 - train loss/token: 1.9202  val loss/token: 1.6535\n",
      "Epoch 4/10 - train loss/token: 1.7531  val loss/token: 1.5482\n",
      "Epoch 5/10 - train loss/token: 1.6437  val loss/token: 1.4766\n",
      "Epoch 6/10 - train loss/token: 1.5655  val loss/token: 1.4176\n",
      "Epoch 7/10 - train loss/token: 1.5040  val loss/token: 1.3829\n",
      "Epoch 8/10 - train loss/token: 1.4558  val loss/token: 1.3437\n",
      "Epoch 9/10 - train loss/token: 1.4164  val loss/token: 1.3132\n",
      "Epoch 10/10 - train loss/token: 1.3818  val loss/token: 1.2994\n",
      "EN: Go.\n",
      "GR: geh los.\n",
      "\n",
      "EN: Hi!\n",
      "GR: [UNK]\n",
      "\n",
      "EN: Good morning.\n",
      "GR: guten morgen [UNK]\n",
      "\n",
      "EN: Good night.\n",
      "GR: gute nacht [UNK]\n",
      "\n",
      "EN: I am fine.\n",
      "GR: ich bin gut.\n",
      "\n",
      "EN: See you later.\n",
      "GR: wir sehen uns später.\n",
      "\n",
      "EN: I forgot my keys at home.\n",
      "GR: ich habe meine schlüssel zu hause vergessen.\n",
      "\n",
      "EN: We are looking for a new apartment.\n",
      "GR: wir suchen eine neue wohnung.\n",
      "\n",
      "EN: She didn’t come because she was sick.\n",
      "GR: sie [UNK] [UNK] weil sie krank war.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "lines = open(\"deu.txt\", encoding=\"utf-8\").read().split(\"\\n\")\n",
    "print(lines[0])\n",
    "print()\n",
    "\n",
    "pairs = []\n",
    "for line in lines:\n",
    "    parts = line.split(\"\\t\")\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "    eng = parts[0].strip()\n",
    "    ger = parts[1].strip()\n",
    "    pairs.append((eng, ger))\n",
    "\n",
    "print(pairs[0])\n",
    "print(len(pairs))\n",
    "print()\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(pairs))\n",
    "\n",
    "MAX_VOCAB = 8000\n",
    "specials = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
    "\n",
    "\n",
    "# Токенізатор (простий)\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "# Будуємо словник\n",
    "def build_vocab(sentences):\n",
    "    counter = Counter()\n",
    "\n",
    "    for s in sentences:\n",
    "        counter.update(tokenize(s))\n",
    "\n",
    "    most_common = counter.most_common(MAX_VOCAB - len(specials))\n",
    "    itos = specials + [w for w, _ in most_common]\n",
    "    stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "    return stoi, itos\n",
    "\n",
    "# Розділяємо дані\n",
    "eng_sentences = [p[0] for p in pairs]\n",
    "ger_sentences = [p[1] for p in pairs]\n",
    "\n",
    "# Створюємо два словники\n",
    "eng_stoi, eng_itos = build_vocab(eng_sentences)\n",
    "ger_stoi, ger_itos = build_vocab(ger_sentences)\n",
    "\n",
    "PAD_IDX = eng_stoi[\"[PAD]\"]    # однакові індекси у двох мовах — добре\n",
    "UNK_IDX = eng_stoi[\"[UNK]\"]\n",
    "BOS_IDX = eng_stoi[\"[BOS]\"]\n",
    "EOS_IDX = eng_stoi[\"[EOS]\"]\n",
    "\n",
    "print(\"ENG vocab size:\", len(eng_stoi))\n",
    "print(\"GER vocab size:\", len(ger_stoi))\n",
    "\n",
    "MAX_LEN_SRC = 20\n",
    "MAX_LEN_TGT = 20\n",
    "\n",
    "\n",
    "def encode(text, stoi, add_specials=False):\n",
    "    tokens = tokenize(text)\n",
    "    ids = []\n",
    "\n",
    "    if add_specials:\n",
    "        ids.append(BOS_IDX)\n",
    "\n",
    "    for t in tokens:\n",
    "        ids.append(stoi.get(t, UNK_IDX))\n",
    "\n",
    "    if add_specials:\n",
    "        ids.append(EOS_IDX)\n",
    "\n",
    "    return ids\n",
    "\n",
    "def prepare_pair(eng, ger):\n",
    "    # encoder input: англ без BOS/EOS\n",
    "    src = encode(eng, eng_stoi, add_specials=False)\n",
    "\n",
    "    # повна цільова послідовність: [BOS ... EOS]\n",
    "    tgt_full = encode(ger, ger_stoi, add_specials=True)\n",
    "\n",
    "    # decoder_inputs: без останнього токена\n",
    "    tgt_in = tgt_full[:-1]\n",
    "\n",
    "    # targets: без першого токена (зсунуті)\n",
    "    tgt_out = tgt_full[1:]\n",
    "\n",
    "    # паддинг\n",
    "    src = src[:MAX_LEN_SRC] + [PAD_IDX] * (MAX_LEN_SRC - len(src))\n",
    "    tgt_in = tgt_in[:MAX_LEN_TGT] + [PAD_IDX] * (MAX_LEN_TGT - len(tgt_in))\n",
    "    tgt_out = tgt_out[:MAX_LEN_TGT] + [PAD_IDX] * (MAX_LEN_TGT - len(tgt_out))\n",
    "\n",
    "    return src, tgt_in, tgt_out\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng, ger = self.pairs[idx]\n",
    "        src, tgt_in, tgt_out = prepare_pair(eng, ger)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(src, dtype=torch.long),\n",
    "            torch.tensor(tgt_in, dtype=torch.long),\n",
    "            torch.tensor(tgt_out, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "\n",
    "# зробимо train/val/test спліт\n",
    "random.shuffle(pairs)\n",
    "\n",
    "total = len(pairs)\n",
    "train_size = int(0.95 * total)\n",
    "\n",
    "train_pairs = pairs[:train_size]\n",
    "val_pairs = pairs[train_size:]\n",
    "\n",
    "print(\"Train:\", len(train_pairs))\n",
    "print(\"Val:\", len(val_pairs))\n",
    "\n",
    "print(\"Total:\", len(train_pairs) + len(val_pairs))\n",
    "\n",
    "train_ds = TranslationDataset(train_pairs)\n",
    "val_ds = TranslationDataset(val_pairs)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "src, tgt_in, tgt_out = next(iter(train_loader))\n",
    "print(\"src:\", src.shape)  # [B, MAX_LEN_SRC]\n",
    "print(\"tgt_in:\", tgt_in.shape)  # [B, MAX_LEN_TGT]\n",
    "print(\"tgt_out:\", tgt_out.shape)\n",
    "\n",
    "\n",
    "def make_padding_mask(x, pad_idx=PAD_IDX):\n",
    "    # x: [B, T]\n",
    "    # True там, де пади (так хоче nn.Transformer)\n",
    "    return (x == pad_idx)\n",
    "\n",
    "\n",
    "def generate_subsequent_mask(size, device):\n",
    "    # True = заборонено\n",
    "    mask = torch.triu(torch.ones(size, size, dtype=torch.bool, device=device), diagonal=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "class TokenPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len=50, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD_IDX)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, T]\n",
    "        B, T = x.shape\n",
    "        pos = torch.arange(T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.token_emb(x) + self.pos_emb(pos)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 d_model=64,\n",
    "                 n_heads=4,\n",
    "                 num_layers=2,\n",
    "                 d_ff=128,\n",
    "                 max_len_src=20,\n",
    "                 max_len_tgt=20,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_emb = TokenPositionalEmbedding(src_vocab_size, d_model, max_len_src, dropout)\n",
    "        self.tgt_emb = TokenPositionalEmbedding(tgt_vocab_size, d_model, max_len_tgt, dropout)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        dec_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output_proj = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt_in):\n",
    "        \"\"\"\n",
    "        src: [B, S]\n",
    "        tgt_in: [B, T]\n",
    "        \"\"\"\n",
    "        src_key_padding_mask = make_padding_mask(src)  # [B, S]\n",
    "        tgt_key_padding_mask = make_padding_mask(tgt_in)  # [B, T]\n",
    "\n",
    "        # embeddings\n",
    "        enc_in = self.src_emb(src)  # [B, S, D]\n",
    "        dec_in = self.tgt_emb(tgt_in)  # [B, T, D]\n",
    "\n",
    "        # encoder\n",
    "        memory = self.encoder(\n",
    "            enc_in,\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [B, S, D]\n",
    "\n",
    "        # декодерська causal mask\n",
    "        T = tgt_in.size(1)\n",
    "        tgt_mask = generate_subsequent_mask(T, device=tgt_in.device)  # [T,T]\n",
    "\n",
    "        dec_out = self.decoder(\n",
    "            dec_in,\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask,\n",
    "        )  # [B, T, D]\n",
    "\n",
    "        logits = self.output_proj(dec_out)  # [B, T, V_tgt]\n",
    "        return logits\n",
    "\n",
    "\n",
    "src_vocab_size = len(eng_stoi)\n",
    "tgt_vocab_size = len(ger_stoi)\n",
    "\n",
    "model = Seq2SeqTransformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=64,\n",
    "    n_heads=4,\n",
    "    num_layers=4,\n",
    "    d_ff=128,\n",
    "    max_len_src=MAX_LEN_SRC,\n",
    "    max_len_tgt=MAX_LEN_TGT,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"Trainable parameters:\", count_parameters(model))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_tokens = 0\n",
    "\n",
    "    for src, tgt_in, tgt_out in loader:\n",
    "        src = src.to(device)\n",
    "        tgt_in = tgt_in.to(device)\n",
    "        tgt_out = tgt_out.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(src, tgt_in)  # [B, T, V]\n",
    "        B, T, V = logits.shape\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.view(B * T, V),\n",
    "            tgt_out.view(B * T)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * (tgt_out != PAD_IDX).sum().item()\n",
    "        n_tokens += (tgt_out != PAD_IDX).sum().item()\n",
    "\n",
    "    return total_loss / n_tokens\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n_tokens = 0\n",
    "\n",
    "    for src, tgt_in, tgt_out in loader:\n",
    "        src = src.to(device)\n",
    "        tgt_in = tgt_in.to(device)\n",
    "        tgt_out = tgt_out.to(device)\n",
    "\n",
    "        logits = model(src, tgt_in)\n",
    "        B, T, V = logits.shape\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.view(B * T, V),\n",
    "            tgt_out.view(B * T)\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item() * (tgt_out != PAD_IDX).sum().item()\n",
    "        n_tokens += (tgt_out != PAD_IDX).sum().item()\n",
    "\n",
    "    return total_loss / n_tokens\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {ep}/{EPOCHS} - train loss/token: {train_loss:.4f}  val loss/token: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "def encode_src_sentence(text, stoi, max_len):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [eng_stoi.get(t, UNK_IDX) for t in tokens]\n",
    "    ids = ids[:max_len] + [PAD_IDX] * (max_len - len(ids))\n",
    "    return torch.tensor(ids, dtype=torch.long).unsqueeze(0)  # [1, S]\n",
    "\n",
    "\n",
    "def decode_tgt_ids(ids, itos):\n",
    "    # ids: список індексів (наприклад, з BOS/EOS/PAD)\n",
    "    tokens = []\n",
    "    for i in ids:\n",
    "        if i in (PAD_IDX, BOS_IDX, EOS_IDX):\n",
    "            continue\n",
    "        tokens.append(itos[i])\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate(model, sentence, max_len=MAX_LEN_TGT):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # 1) кодуємо англійське речення\n",
    "    src = encode_src_sentence(sentence, eng_stoi, MAX_LEN_SRC).to(device)  # [1, S]\n",
    "    src_key_padding_mask = make_padding_mask(src)  # [1, S]\n",
    "\n",
    "    # 2) пропускаємо через encoder\n",
    "    enc_in = model.src_emb(src)  # [1, S, D]\n",
    "    memory = model.encoder(enc_in, src_key_padding_mask=src_key_padding_mask)  # [1, S, D]\n",
    "\n",
    "    # 3) старт декодера з BOS\n",
    "    tgt_ids = [BOS_IDX]\n",
    "    for _ in range(max_len):\n",
    "        tgt_tensor = torch.tensor(tgt_ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "        tgt_key_padding_mask = make_padding_mask(tgt_tensor)\n",
    "\n",
    "        T = tgt_tensor.size(1)\n",
    "        tgt_mask = generate_subsequent_mask(T, device=device)  # [T, T]\n",
    "\n",
    "        dec_in = model.tgt_emb(tgt_tensor)  # [1, T, D]\n",
    "        dec_out = model.decoder(\n",
    "            dec_in,\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask,\n",
    "        )  # [1, T, D]\n",
    "\n",
    "        logits = model.output_proj(dec_out[:, -1, :])  # [1, V]\n",
    "        next_id = logits.argmax(dim=-1).item()\n",
    "\n",
    "        tgt_ids.append(next_id)\n",
    "\n",
    "        if next_id == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    # 4) декодуємо у текст\n",
    "    translation = decode_tgt_ids(tgt_ids, ger_itos)\n",
    "    return translation\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"Go.\",\n",
    "    \"Hi!\",\n",
    "    \"Good morning.\",\n",
    "    \"Good night.\",\n",
    "    \"I am fine.\",\n",
    "    \"See you later.\",\n",
    "    \"I forgot my keys at home.\",\n",
    "    \"We are looking for a new apartment.\",\n",
    "    \"She didn’t come because she was sick.\",\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(\"EN:\", s)\n",
    "    print(\"GR:\", translate(model, s))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208fd1d4-3fd7-467a-b6fe-1c8b657a3286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba11884-6e9b-4658-a0d5-a981553982ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
